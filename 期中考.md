# Lisi
### 1. keyword 是什麼？關鍵字 (或關鍵詞) 又是什麼？你覺得應該怎麼定義「一篇新聞裡的關鍵字是什麼？又該如何取出這些關鍵字？」此外「一篇新聞裡的 keyword 又是什麼？該怎麼取出來？」
在一個data的table裡面，能夠區辨所有data的特徵叫做key，是能把所有人define出來的一個feature，keyword就是可以做為document的key的word，是具有區辨意義的代表字；關鍵字∕詞是意義上最有代表性的重要字。keyword跟關鍵字可以是一樣的字，但代表的意義不同。
一篇新聞裡的關鍵字應是該篇新聞中最具有意義的字，以我的觀點來看，新聞類別的文本中，關鍵字是以動詞為主，因為能夠透過動詞判斷該篇新聞事件主要發生的事件，例如棒球新聞中會有的「安打」以及社會新聞中的「殺」、「撞」等等。keyword可以透過TF-IDF的加權技術摘出，可能會是任何詞性的詞彙，不會限於具有意義的動詞、名詞及形容詞等等，以新聞來說不一定會是關鍵字。

### 2. 請給些例子說明，「什麼是一個字 (word)」？「什麼是一個字符 (character)」？而「什麼是一個「詞 (phrase)」？在 Computer Science 的領域裡，有個專有名詞叫 "token"，依前述的定義，請說明 "token" 和 word, character, phrase 之間有什麼異同。

word(詞)是語言系統之中具有意義的最小單位；character(字符)指的是語言系統中獨立的符號單位；而phrase(我更偏好翻譯成詞組)是word的集合，字句由短到長都有，具有特定意義。例如「蜻」、「蜓」為character(字符)；「蜻蜓」為word(字)；「蜻蜓在花園」為phrase(詞組)
以中文來說，因為有許多character具有獨立意義，所以character也可能是詞；而詞組一定是由一個以上的詞所組成，所以詞組不會是字符或是詞。
token在computer science中指一段不一定有意義的字串，然而以斷詞角度來看的話，為文本的子字串。假如有一個句子是"我看到蜻蜓在花園的水池上飛"，被分成"我"、"看到"、"蜻蜓在花園"、"的"、"水池"、"上飛"，這些都屬於token。是以可以依word、character或是phrase分割成token，又或著都不是，不一定要具有意義，就像上述例子中的"上飛"，即不屬於word、character或是phrase任何一種。


### 3. 只有中文 NLP 有斷詞的需求嗎？其它的語言的 NLP 是否也有斷詞的需求？請以實例 (而非文獻) 說明斷詞是否仍有其存在的必要性。
我認為每個語言的文本都需要經過單詞分割才能夠在電腦中運算，讓電腦先確認不同詞、詞性之間的關聯(如以中文來說「的」這個助詞，後面只會接名詞；或是英文的副詞後面只會接動詞或是形容詞等等)，才能依照該語言的特性去坐進一步的處理（如語意辨析、語言生成、翻譯等等）。
有些書面語言的詞跟詞之間會有分界標記，如西班牙文及英文詞跟詞間的空格、阿拉伯語會有獨特的首末字母等等，然而像是中文、日文等等每句話的字符都連在一起的語言就需要經過特別的斷詞系統設計。
舉例來說：
以中文來說：
「波羅的海是中歐和北歐之間的陸間海。」
如果沒有斷詞系統設定中「波羅的海」這個專有名詞，依照
以日文來說：
ははくちのなかにはえるきかん。(歯は口の中に生える器官)（牙齒是生長在口腔中的器官。）
前面兩個は分分別是名詞及格助詞，如果全都以平假名書寫字的話，沒有經過斷詞也可能被判斷成另外一個名詞はは(母親)。
以英文來說：
Certain people always want to maintain the status quo.
這句話中的status quo是一個中間有空格的名詞，不能單純以空格切開。

### 4. Out-of-vocabulary (OOV) 常是主流斷詞系統評估系統良率的一個指標，常見的說法是「oov rate 愈低，則表示系統愈完備」。請說明這個指標是否正確。
語言會一直創新，以中文為例，參考《不可能還在歸剛誒吧！接軌 Z 世代，2022 網路流行用語大彙集！》（PARTIPOST），2022年新出現的網路流行用語如「無fuck說」、「你彭佳慧唉」、「duck不必」、「安屎之亂」等等……都是從前沒在使用的新詞。
平常跟朋友之間談話也很有可能創造出新詞，如同我最近在朋友群組中看到的語句：「XX你又在偷臭我...就不能好我一下嗎？」，「好」跟「臭」原先都是形容詞，將「臭」當成動詞是2021年在電競圈開始流行的用法，意旨批評或是謾罵，也就是嘴的意思，我的好友就這個句勢，隨手便創造了一個將「好」當成動詞的用法，而且群組內的眾人都能迅速了解它的意義。
除了上述一些非正式書面用語的狀況之外，還有在新聞報導中，因為字數上的限制，一些專有名詞，如人名、機構名稱等，在標題使用或是同一篇文章內如果出現第二次的情況下，就會採用縮寫的方式呈現。只要出現一個新的機構或是一位先前相對沒有這麼有名的人，也會產生新的縮寫詞。
以上的例子，如果使用現行的斷詞系統，不新增字典便無法辨識涵義，因此我認為以OOV率來評估斷詞系統的良率是一個有瑕疵的判斷標準。


### 5. 請說明 WS (word segmentation)、POS (part-of-speech) tag 和 NER (named entity recognition) 各自是什麼？包括 Articut，請再自選兩個斷詞系統，並從應用的角度討論這三個系統的 WS、POS、NER 各自有什麼樣的優缺點（優、缺點請至少各列兩點)。

WS(word segmentation)是指字詞分割，；POS(part-of-speech) tag是詞性標記，為每個單詞指定(標上)相應的詞性；NER (named entity recognition)
Articut：
優：
1. WS、POS及NER標記正確率高 
2. NER種類很多 
3. 可以調整WS深度，以level區分
4. 能夠辨別實義詞(content word) 
缺：
1. 無法處理縮寫詞及長機構名稱

Jieba：
優：
1. WS速度快，新增字典容易
2. WS有三種模式精確、全和搜尋引擎模式，對應不同功能
缺：
1. 無法處理沒有在字典裡的詞(加新詞的字典會影響最終結果，不加新詞又沒辦法處理在字典中沒有的字) 
2. 一開始的模型是用簡體中文去做，繁體中文WS的正確率相對較低 
3. 無法區辨NER標記中不同人名意義的相近程度。

CKIP：
優：
1. 除了WS及POS外還提供了句子的parsing tree  
2. 能夠辨別實義詞(content word) 
3. 除了WS外還能讓詞義的資料視覺化
缺：
1. 資料庫大，WS運算時長久 
2. WS傳輸量有很低的限制(院外一次限兩千字)  
3. WS、POS以及NER的正確率計算指標不明




## 檢討(Peter)：
### 1. keyword 是什麼？關鍵字 (或關鍵詞) 又是什麼？你覺得應該怎麼定義「一篇新聞裡的關鍵字是什麼？又該如何取出這些關鍵字？」此外「一篇新聞裡的 keyword 又是什麼？該怎麼取出來？」

[keyword] 是從 資訊技術 的角度描述文本資料的結果。
[關鍵字] 是從 語意角度 描述文本資料的結果。

偶爾，兩者會是同一組詞，但並不表示兩者的意義是相等的。

由於一般人對領域專有詞的操作定義不理解時，往往會望文生義或是「把中文翻譯成英文/把英文翻譯成中文」就以為理解了，因此一般人會把「keyword」譯為「關鍵字 (或關鍵詞)」便誤以為兩者是一樣的東西了。

要取出一篇新聞的關鍵字(或關鍵詞)，採用的方法應該是「人怎麼理解這則新聞的脈絡，再加以取出語意含量高的詞彙做為該新聞的關鍵字」。

要取出一篇新聞裡的 keyword，採用的方法應該是「該文本內有哪些高頻的 token 剛好是其它文本的低頻  token。那麼這些 token 就適合做為 keyword」。

在實際的工作場合，當長官請你找出「關鍵字」的時候，你要弄明白他的目的是為了「要給人看 (取關鍵字)」還是「為了要給資訊系統使用 (取 keyword)」。

===
### 2. 請給些例子說明，「什麼是一個字 (word)」？「什麼是一個字符 (character)」？而「什麼是一個「詞 (phrase)」？在 Computer Science 的領域裡，有個專有名詞叫 "token"，依前述的定義，請說明 "token" 和 word, character, phrase 之間有什麼異同。

一個字 (word) 是句法節點上的最小單位。句法樹 (syntax tree) 是計算語意的基本框架。不符合句法的 [東西]，無法表達語意。(e.g., 「椅一子把我天坐看」)

因為「字」的定義涉及語意，而資訊領域對句法 (syntax) 又不太瞭解，因此資訊領域往往簡化定義為「字是表達語言含義的最小單位」。

較好的操作型定義，應該是如上所述， 透過「句法」加以規範，再計算其「語意」。能在這個句子裡表達語意的，才算是一個字。

舉例來說，「球場」如果視為是兩個字，那麼一個沒有球的球場是不是就只能叫稱之為「場」？否則「球」的意義會跑進句子裡來。依此推估，那麼第一場比賽開始以前，我們只能說「我們蓋了一座場」而不能說「我們蓋了一座球場」。

此外，如果「球場」是兩個字，那麼我們也無法解釋為什麼中文的量詞* 系統用在這裡時稱之為「一座球場」而不稱之為「一顆球場」(既然這裡也有「球」這個字嘛，既然 「字是表達語言含義的最小單位」 那表示「球」在這裡也有語意呀，那你為什麼要憑「場」來決定量詞要用「座」？)

相反地，如果我們從句法結構的角度來看「我們蓋了一座球場」，就能避免要解釋 「球」去哪裡？ 的問題，因為在句法的結構下，有意義的字指的是「球場」而不是「球」，也就是說「球場」在這個句子裡才是一個字，而不是「球」在這個句子裡是一個字。

因此，什麼是「一個字 (word)」，要從句子的角度切入，定義為「一個字 (word) 是句法節點上的最小單位。」而不能過度簡化地說「字是表達語言含義的最小單位」。

[註] 我在這裡先不區分「量詞」和「分類詞」。

中文的「字(word)」和「詞 (phrase)」都是句法節點上節點。差別只在「字」是最小節點，而「詞」是較高的節點。一個「詞」通常是由「多個字」組成。

例如
「很 (word)+ 迅速(word)」= 很迅速 (phrase)
「看(word) + 過 (word)」= 看過 (phrase)

如果要比 word 更小的話，則脫離句法的層次，進入構詞的層詞 (morphology)。

在資訊系統上，一個 ASCII 或 Unicode 字碼表上編定的符號稱為一個字符。只是剛好，在 CJK (Chinese, Japanese, Korean) 區段裡，每個字符 (character) 在這些語言中的某些句子裡，也可以做為一個字 (或詞) 使用。因此中文母語者常常分不清楚「字、詞」有什麼不一樣。

簡單地說，正確的分類法是：
- phrase: 詞 (或稱「詞組」)
- word: 字
- character: 字符、字元

相對地，常見的困惑分類法是：
- phrase: 詞
- word: 字、詞
- character: 字、字符、字元

從上面的分類可以看到「字」會讓人覺得困惑，是因為它佔了兩個分類項目 (那就是分不清楚的意思)，而「詞」也被放在兩個類別裡，再加上「字」和「詞」又不定義清楚。定義不明確，就難以做溝通，更別說要合作設計可以操作它的系統了。

token 的定義就簡單多了，它只是一個資訊系統用的詞彙。任何「從長字串裡取出的一部份」都叫 token。

### 3. 只有中文 NLP 有斷詞的需求嗎？其它的語言的 NLP 是否也有斷詞的需求？請以實例 (而非文獻) 說明斷詞是否仍有其存在的必要性。

不只中文斷詞的需求。其它語言也都有斷詞的需求。事實上，人類大腦裡的語言系統就是一直在做斷詞的事情。因為「語言指的是語音，而非文字」而「語音裡的詞彙之間是連續，而非有邊界標記的」。

這表示，人類的大腦裡本來就有「找出詞彙邊界」的機制。

所以，任何語言的處理 (any natural language processing) 都涉及「斷詞」的需求。

這也對任何說「我們採用一個不需要斷詞的 NLP  (a.k.a. Google BERT)…」的技術提出靈魂之問：「那你們是在做 token processing？還是在做 language processing？」

承上題的說明，token 是「從長字串裡取出的一部份」都叫  token。那麼它有可能取到 character, 也可能取到 word, phrase ...等不同的層次。換言之，如果它剛好取得比較多語言的成份 (e.g., word, phrase) 那麼它在那一句的處理會比較好。如果剛好取到比較多 character 的成份時，那麼它在那一句的處理就會比較差。

不論是較好，或是較差，總之它都不是在做 language processing 呀！

### 4. Out-of-vocabulary (OOV) 常是主流斷詞系統評估系統良率的一個指標，常見的說法是「oov rate 愈低，則表示系統愈完備」。請說明這個指標是否正確。

這題大家的理解都是完全正確的。我任何一人的答案摘錄如下：
OOV 新詞，是指「沒有看過的詞彙」，若一個斷詞系統背後建立大量字典，則該系統OOV rate 會較低。

但事實上，一個好的斷詞系統 OOV rate 應為追求越高越好，表示系統在較多沒有看過的詞彙下，也能有較低的錯誤率

### 5. 請說明 WS (word segmentation)、POS (part-of-speech) tag 和 NER (named entity recognition) 各自是什麼？包括 Articut，請再自選兩個斷詞系統，並從應用的角度討論這三個系統的 WS、POS、NER 各自有什麼樣的優缺點（優、缺點請至少各列兩點)。

關於 CKIP 的商用，仍需付費從中研院取得商用授權的部份，在昨天的活動中已經說明了。其它的問題，我分項說明如下：

CKIP:
誤解：
可同時進行斷詞、實體辨識、語義辨識、指代消解、關係抽取和剖析
正解：
CKIP 的斷詞、詞性、實體辨識、語意辨識、指代消解和關系抽取剖析是「分段完成」，而不是「同時進行」的。網頁上只是「全部完成以後，再一次給你看結果」而已。

誤解：
因爲不斷加入新詞，導致任務速度緩慢
正解：
CKIP 自己沒有加新詞。使用者可以加新詞，但它要和它自己原來有的模型做一次新的分佈計算來產生新的模型。這個步驟很花時間，因為你加了詞以後，模型能不能用，要等它新模型生出來以後，再試試看。如果不行，那麼前述動作要再做一次，直到可以用為止。

誤解：
第一個具有未知詞偵測和句法預測能力的斷詞系統
正解：
CKIP classic 就有這樣的能力了。事券上，未知詞偵測能力甚至在 Jieba 的時代就有了。

誤解：
CKIP 的詞類標記功能不只解決詞類歧異且可預測新詞的詞類
正解：
CKIP 的詞類標記功能和轉品能力和 Articut 相比是較弱的。所以在詞庫小組主持人馬偉雲老師受訪的文章裡才會提到「這條河很難過」和「某人很難過」的兩個「難過」在 CKIP 裡分不出來。

---
關於 Jieba 的部份：
誤解：
tfidf使用的 corpus 為1998年的人民日報，會有偏誤，例如：講台/中央、造成台灣地名切錯 台/中
Jieba 不只是 TF-IDF 使用的是 98 年的人民日報，它整個統計模型的底子都是當時的人民日報。

---
關於 Articut  的部份：
誤解：
背後的語言學理論線上學習資源不足
正解：
理論語言學還真的很不容易在線上完成學習 (苦笑)。

因為它是一門「用語言在解釋語言」的學科。試想，我們可以線上學習數學，是因為我們可以「用語言解釋數學」，如果我們沒有語言，而只能「用數字解釋數字, 甚至連∵和∴這樣的邏輯因果符號都不能使用的話」，那麼數學的線上學習也會變得非常困難。

這是語言學難以變成線上課程的主要原因。

然而，它還是有很多線上課程，但絕大多數都是英文的。而主流歐美語言的文字系統面臨的 NLP 問題，相較之下比中文簡單一些 (note. 他們的語言絕大多數都有相對豐富的詞綴)。

因此結合了「在線上難以用語言描述語言」和「中文資源較少」的條件下，我們才決定推動這個緊湊的暑期學習/實習活動。

誤解：
因為是以語言結構來斷詞，因此較能識別出流行語和表情符號等新詞彙
正解：
Articut 可以透過語言結構來處理新詞、流行語，但Articut 把所有的 emoji 都當成是 oov；此外，其它的顏文字則視為一串符號處理。所以其實 Articut 是沒有處理表情符號的。

誤解：
斷詞結果為「球場缺失」，應分為「球場」、「缺失」。
正解：
假設在「工會曾提27項球場缺失」一句中，採用 lv2 詞組斷詞 (取 phrase 節點)，則輸出裡的確是「球場缺失」因為對其前的「27項」量詞而言，它計算的是「球場缺失」。因此「球場缺失」合在一起才是正確的輸出結果。

相較之下，如果對同一個句子，採用 lv1 極致斷詞 (取 word 節點)，則輸出的結果裡就是「球場」「缺失」了。所以，想要取得「比較長的詞彙，較能代表句子的意義」或是想取得「比較短的詞彙，較能計算詞彙的意義」完全是看使用者自己的需求和設定。 

Articut 提供 lv1, lv2 兩層輸出結果的設計目的即在此。

誤解：
 Articut 的 NER 很臺灣 (列為缺點)
正解：
我以為這是優點！(笑)

Articut 是 NLP 技術的發展中第一次出現「有一個技術解決方案完全是台灣獨立開發。而且開發完以後的系統原理，也可以應用到其它語言 (我們做英文版就是為了證明這點)」的事情。

以往所有的 NLP 技術、工具、模組都是外國人先發明，然後我們再拿他們的工具、模型、框架、理論、論文裡的系統架構…等等，依樣畫葫蘆地弄個中文的版本。像 Articut 這樣每一行 code 都是源自台灣的，是技術史上的第一次。(我們甚至全部用原生的 Python 模組而已，連 numpy, scipy…都沒用到呢。要再更底層的話，就只有重新發明一個程式語言來寫 Articut 了。)

畢竟，即便是 CKIP，它底子裡也用了非常大量的簡體中文文本做為訓練資料。所以它的模型是一個「傾向簡中(畢竟這部份的資料量比較大) 又兼具繁中 (因為繳稅買單的是這裡的人) 處理能力」的模型。

不然，大家可以試試看直接把人民日報裡的簡中新聞貼上 CKIP Core NLP 的網頁試試，效果相當不錯呢！(這就是 Jessie 問的「如果對方不說，你怎麼知道人家的背後用了什麼方法？」的測試了。)

---
HanLP
理解正確
mmseg
理解正確
Stanford Core NLP
理解正確
monpa
理解正確
